{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMnBiFEAnIociT6Ft5kfo84",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/robert-pineau/CIND-860-Capstone/blob/main/CIND860_build_numpy_batches.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rxTycRNwzA5R"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "import glob\n",
        "import re\n",
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "import random\n",
        "import cv2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image_dir = \"/mnt/wd/CIND860/database/square_cc_images\"\n",
        "dataset_file = \"/mnt/wd/CIND860/train.csv\""
      ],
      "metadata": {
        "id": "TVcBra1LzEKh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_loadable_data(indexes, df, cnn_use, final_dir, batch_size=12):\n",
        "  #Calculate how many batches total.\n",
        "  data_len = int(np.floor(len(indexes) / batch_size))\n",
        "\n",
        "  #Cycle through each batch\n",
        "  for n in range(0,data_len):\n",
        "\n",
        "    #Initialize the arrays used to hold this batch.\n",
        "    X = np.zeros([0,224,224,3])\n",
        "    Y = []\n",
        "    Z = []\n",
        "\n",
        "    #Calculate starting and ending ID for this batch.\n",
        "    start = int(n*batch_size)\n",
        "    end = int((n+1)*batch_size)\n",
        "\n",
        "    #Cycle through each image for this batch\n",
        "    for i in range(start,end):\n",
        "      png_name = indexes[i]\n",
        "\n",
        "      #Extract use(train, validate, test) from the image name, plus the patient-id and the image-id.\n",
        "      results = re.search(r\"((train)|(test)|(validate))\\/(\\d+)\\_(\\d+)(\\_(\\d+)){0,1}\\.png\", png_name)\n",
        "      patient_id = int(results[5])\n",
        "      image_id = int(results[6])\n",
        "\n",
        "      #Read image into a numpy array using cv2\n",
        "      img = cv2.imread(png_name)\n",
        "\n",
        "      #Resize image as 224x224\n",
        "      img = cv2.resize(img, (224, 224))\n",
        "\n",
        "      #convert images to floats, and rescale from 0-1\n",
        "      img = cv2.normalize(img, None, 0, 1.0, cv2.NORM_MINMAX, dtype=cv2.CV_32F)\n",
        "\n",
        "      #Extract class variable (cancer) for this image.\n",
        "      df_index = int(np.where(df['image_id'] == image_id)[0])\n",
        "      cancer = df.at[df_index,'cancer']\n",
        "\n",
        "      #Append image, Class(cancer), and image-id into three separate arrays\n",
        "      #X,Y used to train/validate and test.\n",
        "      #Z used to be able to retrieve image-id after the fact to see\n",
        "      #exactly which images the final algorithm works on, and which it does not work on.\n",
        "\n",
        "      X = np.append(X,[img],axis=0)\n",
        "      Y = np.append(Y,[cancer],axis=0)\n",
        "      Z = np.append(Z,[image_id],axis=0)\n",
        "\n",
        "    #Save the three arrays into a file, one for each variable(X,Y,Z) and each batch(of 12 images).\n",
        "    np.save(f\"{final_dir}/{cnn_use}_data_X_{n}\",X)\n",
        "    np.save(f\"{final_dir}/{cnn_use}_data_Y_{n}\",Y)\n",
        "    np.save(f\"{final_dir}/{cnn_use}_data_Z_{n}\",Z)\n"
      ],
      "metadata": {
        "id": "GIOiWPLIzHF6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Read in the database file for details regarding each image/patient.\n",
        "df=pd.read_csv(dataset_file,sep=',')"
      ],
      "metadata": {
        "id": "0LsB7eH8zvlm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Cycle through all png files\n",
        "#Find all files, and the file name into\n",
        "#appropriate list for train, validate, and test.\n",
        "#\n",
        "glob_string = f\"{image_dir}/*/*.png\"\n",
        "image_list = glob.glob(os.path.join(\"\", glob_string))\n",
        "random.shuffle(image_list)\n",
        "\n",
        "#Initialize lists to hold image names for each category (train/validate/test)\n",
        "train_ids = []\n",
        "validate_ids = []\n",
        "test_ids = []\n",
        "\n",
        "#Cycle through every image\n",
        "for png_name in image_list:\n",
        "  results = re.search(r\"((train)|(test)|(validate))\\/(\\d+)\\_(\\d+)(\\_(\\d+)){0,1}\\.png\", png_name)\n",
        "  cnn_use = results[1]\n",
        "\n",
        "  #assign image name into proper category list\n",
        "  if cnn_use == \"train\":\n",
        "    train_ids = np.append(train_ids,[png_name],axis=0)\n",
        "  elif cnn_use == \"validate\":\n",
        "    validate_ids = np.append(validate_ids,[png_name],axis=0)\n",
        "  elif cnn_use == \"test\":\n",
        "    test_ids = np.append(test_ids,[png_name],axis=0)\n",
        "\n",
        "\n",
        "#Final shuffle within each category before splitting them up into batches\n",
        "random.shuffle(train_ids)\n",
        "random.shuffle(validate_ids)\n",
        "random.shuffle(test_ids)\n",
        "\n",
        "print(train_ids.shape)\n",
        "print(validate_ids.shape)\n",
        "print(test_ids.shape)"
      ],
      "metadata": {
        "id": "wSUplJeDzLep"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_dir = f\"{image_dir}/numpy\"\n",
        "\n",
        "#Take each list(train, validate, test), and create numpy files for each variable, and batch.\n",
        "create_loadable_data(train_ids, df, \"train\", final_dir, batch_size=12)\n",
        "create_loadable_data(validate_ids, df, \"validate\", final_dir, batch_size=12)\n",
        "create_loadable_data(test_ids, df, \"test\", final_dir, batch_size=12)\n",
        "\n",
        "#Tar and compress(gzip) the numpy files for transfer.\n",
        "#This is needed, as the CNN training runs significantly faster when loading numpy files\n",
        "#from the google colab /tmp directory, then it does when running from a\n",
        "#mounted google drive.\n",
        "#\n",
        "#However, the file transfer is 12x faster if the files are compressed before transfer.\n",
        "#(GZIP provides a 25x compression on numpy image data for the train dataset, 28GB vs 1.1GB)\n",
        "os.system(f\"tar -cpzvf {image_dir}/numpy/train.tgz {image_dir}/numpy/train*.npy\")\n",
        "os.system(f\"tar -cpzvf {image_dir}/numpy/validate.tgz {image_dir}/numpy/validate*.npy\")\n",
        "os.system(f\"tar -cpzvf {image_dir}/numpy/test.tgz {image_dir}/numpy/test*.npy\")"
      ],
      "metadata": {
        "id": "pDeHRfDK08SF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}